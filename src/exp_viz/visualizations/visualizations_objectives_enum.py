from enum import Enum

class VisualizationsObjectivesEnum(Enum):
    """Class that stores the explanations of the objectives of each visualization as an Enum."""

    SHAP_BAR_PLOT = ("The global bar plot shows the features used by to train the model in "
                     "the y-axis and their mean absolute SHAP values in the x-axis. The "
                     "features are ordered by the values represented by the bars, which "
                     "means that it is ordered by most impactful to least impactful feature. "
                     "This makes it a useful visualization to understand which were the most "
                     "important features for the model. However, we do not know by this "
                     "visualization if the feature had a positive or a negative impact. On "
                     "the other hand, the local bar plot represents the SHAP values of each "
                     "feature for a specific predicted instance. That means that we are "
                     "able to understand how much each feature impacted to that specific "
                     "output and also if that contribution had a positive or negative "
                     "impact. Neither of these visualizations inform how higher or "
                     "lower values of the feature impact the predictions or the model.")

    SHAP_BEESWARM_PLOT = ("The beeswarm plot is a visualization only for global explanations. It "
                          "shows the features in the y-axis ordered by their impact in the model's "
                          "output and also display a vertical bar with a color range of the "
                          "features values from high to low. The distribution of the features' "
                          "SHAP values is plotted along the x-axis and the dots are colored "
                          "according to the before mentioned color range. That makes this "
                          "visualization useful when there is a need to understand the model's "
                          "behavior as a whole and to know how the higher and lower values of "
                          "each feature impact the output.")

    SHAP_WATERFALL_PLOT = ("The waterfall plot supports only local explanations. In this plot, the "
                           "x-axis has the possible values for the target variable instead of the "
                           "SHAP values, and the y-axis has the features. In the x-axis there is "
                           "also the representation of the target expected value, which is the "
                           "mean value of all the predictions, and shows how much each feature "
                           "contributed for that specific prediction to have been higher or lower "
                           "than the expected value. It is a useful visualization if we want to "
                           "understand if a feature had a positive or a negative impact on the "
                           "prediction and how strong that impact was.")

    LIME_PLOT = ("The visualization for LIME explanations support only local explanations and is "
                 "divided in three parts. The first one, shows the target classes of the problem "
                 "and the probability generated by the model for each of them. The second shows "
                 "the most important features, ordenated from most important to least important, "
                 "showing how much each of them contributed to the final probabilities and for "
                 "which class they contributed. It also gives the conditions from the locally "
                 "trained model that were activated by the feature value along with their "
                 "contribution. The third part gives the list of features, also ordered by "
                 "importance, and their values in the predicted instance, coloring each line "
                 "with the color that represents the class that they contributed to. This "
                 "visualization gives a lot of information and can be used for various objectives. "
                 "As it displays the conditions that were activated by the features' values in "
                 "that instance, it is possible to understand how the model behaves around that "
                 "instance. It can also be useful if we want to understand how each feature impacts"
                 "that specific prediction and also to which target class each of the features "
                 "contributed.")
